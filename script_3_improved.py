# éšæ®µ3: æ”¹è‰¯ç‰ˆå…§å®¹åˆ†æèˆ‡çµæ§‹åŒ– - æ•´åˆStockTitan + Geminiè³‡æ–™
# ä¸‰å±¤å…§å®¹é‡‘å­—å¡”æ¡†æ¶ (What/Why/So What)

import json
import logging
import re
import os
from typing import Dict, Any, List, Optional
from datetime import datetime
from dataclasses import dataclass

# å¦‚æœéœ€è¦é¡å¤–çš„Geminiåˆ†æ
import google.generativeai as genai
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO')),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('YourPods_ContentAnalysis')

@dataclass
class AnalysisConfig:
    """å…§å®¹åˆ†æé…ç½®"""
    
    def __init__(self):
        self.gemini_api_key = os.getenv('GEMINI_API_KEY')
        self.use_gemini_enhancement = os.getenv('USE_GEMINI_ENHANCEMENT', 'true').lower() == 'true'
        
        # Geminié…ç½® (å¦‚æœéœ€è¦é¡å¤–åˆ†æ)
        if self.gemini_api_key and self.use_gemini_enhancement:
            genai.configure(api_key=self.gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
        else:
            self.gemini_model = None
        
        logger.info("âœ… YourPodså…§å®¹åˆ†æé…ç½®è¼‰å…¥å®Œæˆ")

class ImprovedContentAnalyzer:
    """æ”¹è‰¯ç‰ˆå…§å®¹åˆ†æèˆ‡çµæ§‹åŒ–è™•ç†å™¨ - æ•´åˆStockTitan + Geminiæ•¸æ“š"""
    
    def __init__(self):
        self.config = AnalysisConfig()
        self.analysis_templates = self._load_analysis_templates()
        logger.info("ğŸ§  YourPodsæ”¹è‰¯ç‰ˆå…§å®¹åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def process(self, information_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸»è¦è™•ç†å‡½æ•¸ - èˆ‡åŸæœ¬script_3.pyå®Œå…¨ç›¸å®¹
        
        Args:
            information_data: ä¾†è‡ªscript_2_improved.pyçš„è³‡è¨Šæ•¸æ“š
            
        Returns:
            ä¸‰å±¤é‡‘å­—å¡”çµæ§‹çš„åˆ†æçµæœ
        """
        ticker = information_data.get('ticker')
        logger.info(f"ğŸ¯ [YourPods] é–‹å§‹ä¸‰å±¤é‡‘å­—å¡”åˆ†æ: {ticker}")
        
        try:
            # 1. æ™ºèƒ½æå–é—œéµè³‡è¨Š (è™•ç†æ–°çš„è³‡æ–™æ ¼å¼)
            key_info = self._extract_enhanced_information(information_data)
            
            # 2. åŸ·è¡Œä¸‰å±¤é‡‘å­—å¡”åˆ†æ
            layer_1 = await self._analyze_what_layer(key_info)
            layer_2 = await self._analyze_why_layer(key_info, layer_1)
            layer_3 = await self._analyze_so_what_layer(key_info, layer_1, layer_2)
            
            # 3. å“è³ªæª¢æŸ¥å’Œä¿¡å¿ƒè©•ä¼°
            quality_assessment = self._assess_analysis_quality(layer_1, layer_2, layer_3, key_info)
            
            # 4. æ•´åˆGeminiå°ˆæ¥­åˆ†æ (å¦‚æœå¯ç”¨)
            enhanced_analysis = await self._enhance_with_gemini(key_info, layer_1, layer_2, layer_3)
            
            result = {
                "status": "success",
                "ticker": ticker,
                "layer_1_what": layer_1,
                "layer_2_why": layer_2,
                "layer_3_so_what": layer_3,
                "enhanced_analysis": enhanced_analysis,  # æ–°å¢çš„å¢å¼·åˆ†æ
                "analysis_metadata": {
                    "confidence_level": quality_assessment["confidence"],
                    "data_completeness": quality_assessment["completeness"],
                    "key_assumptions": quality_assessment["assumptions"],
                    "analysis_timestamp": datetime.now().isoformat(),
                    "source_method": "StockTitan_Gemini_Enhanced",
                    "pyramid_framework": "What_Why_SoWhat"
                }
            }
            
            logger.info(f"âœ… {ticker} ä¸‰å±¤é‡‘å­—å¡”åˆ†æå®Œæˆ - ä¿¡å¿ƒåº¦: {quality_assessment['confidence']}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ {ticker} å…§å®¹åˆ†æå¤±æ•—: {str(e)}")
            return {
                "status": "error",
                "ticker": ticker,
                "error_message": str(e),
                "partial_analysis": None,
                "analysis_metadata": {
                    "analysis_timestamp": datetime.now().isoformat(),
                    "error": True
                }
            }
    
    def _extract_enhanced_information(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æå–ä¸¦æ•´åˆä¾†è‡ªStockTitan + Geminiçš„è³‡è¨Š"""
        
        structured_data = data.get('structured_data', {})
        gemini_analysis = data.get('gemini_professional_analysis', {})
        raw_information = data.get('raw_information', [])
        
        # æ•´åˆæ‰€æœ‰è³‡è¨Šä¾†æº
        enhanced_info = {
            "ticker": data.get('ticker'),
            "collection_metadata": data.get('collection_metadata', {}),
            
            # ä¾†è‡ªStockTitançš„çµæ§‹åŒ–æ•¸æ“š
            "price_data": structured_data.get('price_data', []),
            "news_events": structured_data.get('news_events', []),
            "analyst_opinions": structured_data.get('analyst_opinions', []),
            "market_context": structured_data.get('market_context', []),
            "company_fundamentals": structured_data.get('company_fundamentals', []),
            
            # ä¾†è‡ªGeminiçš„å°ˆæ¥­åˆ†æ
            "gemini_professional_analysis": gemini_analysis.get('professional_analysis', ''),
            "gemini_success": gemini_analysis.get('success', False),
            
            # Rhea-AIå°ˆæ¥­æ•¸æ“š
            "rhea_ai_insights": self._extract_rhea_ai_insights(raw_information),
            
            # æ•´åˆçš„åŸå§‹å…§å®¹
            "consolidated_content": self._consolidate_enhanced_content(raw_information),
            
            # å…§å®¹å“è³ªæŒ‡æ¨™
            "content_quality_scores": self._calculate_content_scores(structured_data, gemini_analysis, raw_information)
        }
        
        logger.info(f"ğŸ“Š è³‡è¨Šæ•´åˆå®Œæˆ - Geminiåˆ†æ: {'âœ…' if enhanced_info['gemini_success'] else 'âŒ'}, "
                   f"AIæ´è¦‹: {len(enhanced_info['rhea_ai_insights'])}å€‹")
        
        return enhanced_info
    
    def _extract_rhea_ai_insights(self, raw_information: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–æ‰€æœ‰Rhea-AIå°ˆæ¥­æ´è¦‹"""
        
        ai_insights = []
        
        for item in raw_information:
            if item.get('success') and item.get('source') == 'StockTitan_Professional':
                rhea_data = item.get('rhea_ai_analysis', {})
                
                if rhea_data and any(rhea_data.values()):
                    ai_insights.append({
                        'summary': rhea_data.get('summary', ''),
                        'sentiment': rhea_data.get('sentiment', ''),
                        'impact': rhea_data.get('impact', ''),
                        'end_of_day': rhea_data.get('end_of_day', ''),
                        'tags': rhea_data.get('tags', []),
                        'source_url': item.get('url', ''),
                        'timestamp': item.get('timestamp', ''),
                        'quality_score': item.get('quality_score', 0)
                    })
        
        return ai_insights
    
    def _consolidate_enhanced_content(self, raw_information: List[Dict[str, Any]]) -> str:
        """æ•´åˆå¢å¼·ç‰ˆåŸå§‹å…§å®¹"""
        
        content_pieces = []
        
        # å„ªå…ˆç´šæ’åºï¼šStockTitan > Backup > Others
        sorted_info = sorted(raw_information, 
                           key=lambda x: (
                               x.get('source') == 'StockTitan_Professional',
                               x.get('quality_score', 0)
                           ), 
                           reverse=True)
        
        for item in sorted_info:
            if item.get('success'):
                relevant_content = item.get('relevant_content', '')
                if relevant_content:
                    source = item.get('source', 'Unknown')
                    quality = item.get('quality_score', 0)
                    content_pieces.append(f"=== {source} (å“è³ª: {quality:.1f}) ===\n{relevant_content}")
        
        return '\n\n'.join(content_pieces)
    
    def _calculate_content_scores(self, structured_data: Dict[str, Any], 
                                 gemini_analysis: Dict[str, Any], 
                                 raw_information: List[Dict[str, Any]]) -> Dict[str, float]:
        """è¨ˆç®—å…§å®¹å“è³ªåˆ†æ•¸"""
        
        scores = {
            "overall_quality": 0.0,
            "data_richness": 0.0,
            "ai_analysis_quality": 0.0,
            "source_diversity": 0.0
        }
        
        # è³‡æ–™è±å¯Œåº¦
        total_items = sum(len(structured_data.get(key, [])) for key in structured_data)
        scores["data_richness"] = min(total_items / 10.0, 1.0)  # 10å€‹é …ç›®ç‚ºæ»¿åˆ†
        
        # AIåˆ†æå“è³ª
        if gemini_analysis.get('success'):
            analysis_length = len(gemini_analysis.get('professional_analysis', ''))
            scores["ai_analysis_quality"] = min(analysis_length / 2000.0, 1.0)  # 2000å­—ç‚ºæ»¿åˆ†
        
        # ä¾†æºå¤šæ¨£æ€§
        successful_sources = len([item for item in raw_information if item.get('success')])
        scores["source_diversity"] = min(successful_sources / 5.0, 1.0)  # 5å€‹ä¾†æºç‚ºæ»¿åˆ†
        
        # æ•´é«”å“è³ª
        scores["overall_quality"] = (
            scores["data_richness"] * 0.3 +
            scores["ai_analysis_quality"] * 0.4 +
            scores["source_diversity"] * 0.3
        )
        
        return scores
    
    async def _analyze_what_layer(self, key_info: Dict[str, Any]) -> Dict[str, Any]:
        """ç¬¬ä¸€å±¤åˆ†æï¼šWhat - äº‹å¯¦å±¤"""
        
        logger.info("ğŸ“Š åˆ†æç¬¬ä¸€å±¤: What (äº‹å¯¦å±¤)")
        
        # è­˜åˆ¥æ ¸å¿ƒå‚¬åŒ–åŠ‘
        core_catalyst = self._identify_enhanced_catalyst(key_info)
        
        # æå–é—œéµæŒ‡æ¨™
        key_metrics = self._extract_enhanced_metrics(key_info)
        
        # æå–å®˜æ–¹è²æ˜
        official_statements = self._extract_enhanced_statements(key_info)
        
        return {
            "core_catalyst": core_catalyst,
            "key_metrics": key_metrics,
            "official_statements": official_statements,
            "data_sources_used": self._count_data_sources(key_info)
        }
    
    async def _analyze_why_layer(self, key_info: Dict[str, Any], layer_1: Dict[str, Any]) -> Dict[str, Any]:
        """ç¬¬äºŒå±¤åˆ†æï¼šWhy - æ•˜äº‹å±¤"""
        
        logger.info("ğŸ” åˆ†æç¬¬äºŒå±¤: Why (æ•˜äº‹å±¤)")
        
        # åˆ†æå¸‚å ´æ•˜äº‹
        market_narrative = self._analyze_enhanced_narrative(key_info, layer_1)
        
        # åŒæ¥­æ¯”è¼ƒ
        peer_comparison = self._analyze_enhanced_comparison(key_info)
        
        # åˆ†æå¸«å…±è­˜
        analyst_consensus = self._analyze_enhanced_consensus(key_info)
        
        return {
            "market_narrative": market_narrative,
            "peer_comparison": peer_comparison,
            "analyst_consensus": analyst_consensus,
            "narrative_confidence": self._calculate_narrative_confidence(key_info)
        }
    
    async def _analyze_so_what_layer(self, key_info: Dict[str, Any], 
                                   layer_1: Dict[str, Any], 
                                   layer_2: Dict[str, Any]) -> Dict[str, Any]:
        """ç¬¬ä¸‰å±¤åˆ†æï¼šSo What - æ´è¦‹å±¤"""
        
        logger.info("ğŸ’¡ åˆ†æç¬¬ä¸‰å±¤: So What (æ´è¦‹å±¤)")
        
        # å¤šç©ºè«–æˆ°
        bull_case = self._identify_enhanced_bull_case(key_info, layer_1, layer_2)
        bear_case = self._identify_enhanced_bear_case(key_info, layer_1, layer_2)
        
        # å‰ç»å±•æœ›
        forward_outlook = self._generate_enhanced_outlook(key_info, layer_1, layer_2)
        
        # é¢¨éšªè©•ä¼°
        risk_factors = self._assess_enhanced_risks(key_info, layer_1, layer_2)
        
        return {
            "bull_case": bull_case,
            "bear_case": bear_case,
            "forward_outlook": forward_outlook,
            "risk_factors": risk_factors,
            "investment_thesis_strength": self._calculate_thesis_strength(key_info, layer_1, layer_2)
        }
    
    def _identify_enhanced_catalyst(self, key_info: Dict[str, Any]) -> str:
        """å¢å¼·ç‰ˆå‚¬åŒ–åŠ‘è­˜åˆ¥ - åˆ©ç”¨Rhea-AIå’ŒGeminiåˆ†æ"""
        
        # å„ªå…ˆä½¿ç”¨Rhea-AIçš„summary
        ai_insights = key_info.get('rhea_ai_insights', [])
        if ai_insights:
            primary_summary = ai_insights[0].get('summary', '')
            if primary_summary:
                return f"æ ¸å¿ƒå‚¬åŒ–åŠ‘: {primary_summary[:200]}..."
        
        # å…¶æ¬¡ä½¿ç”¨Geminiåˆ†æ
        gemini_analysis = key_info.get('gemini_professional_analysis', '')
        if gemini_analysis:
            # æå–ç¬¬ä¸€å€‹é‡è¦äº‹ä»¶
            catalyst_match = re.search(r'æ ¸å¿ƒäº‹ä»¶[ï¼š:]\s*([^ã€‚\n]{20,100})', gemini_analysis)
            if catalyst_match:
                return f"æ ¹æ“šAIåˆ†æ: {catalyst_match.group(1)}"
        
        # æœ€å¾Œä½¿ç”¨é—œéµå­—åŒ¹é…
        consolidated_content = key_info.get('consolidated_content', '').lower()
        
        catalyst_patterns = {
            "è²¡å ±ç™¼å¸ƒ": ["earnings", "quarterly", "revenue", "profit", "guidance", "è²¡å ±", "å­£å ±"],
            "ä½µè³¼æ¶ˆæ¯": ["merger", "acquisition", "deal", "takeover", "ä½µè³¼", "æ”¶è³¼"],
            "ç”¢å“ç™¼å¸ƒ": ["product", "launch", "release", "innovation", "ç”¢å“", "ç™¼å¸ƒ"],
            "ç›£ç®¡è®ŠåŒ–": ["fda", "approval", "regulation", "policy", "ç›£ç®¡", "æ‰¹å‡†"],
            "ç®¡ç†å±¤è®Šå‹•": ["ceo", "executive", "management", "leadership", "ç®¡ç†å±¤", "é«˜ç®¡"]
        }
        
        for catalyst_type, keywords in catalyst_patterns.items():
            if any(keyword in consolidated_content for keyword in keywords):
                return f"æª¢æ¸¬åˆ°{catalyst_type}ç›¸é—œçš„é‡è¦äº‹ä»¶"
        
        return "æœªç™¼ç¾æ˜ç¢ºçš„ç‰¹å®šå‚¬åŒ–åŠ‘ï¼Œå»ºè­°é—œæ³¨æ•´é«”å¸‚å ´è¡¨ç¾"
    
    def _extract_enhanced_metrics(self, key_info: Dict[str, Any]) -> Dict[str, str]:
        """å¢å¼·ç‰ˆé—œéµæŒ‡æ¨™æå–"""
        
        metrics = {
            "price_change": "æ•¸æ“šæ”¶é›†ä¸­",
            "volume_analysis": "æ•¸æ“šæ”¶é›†ä¸­",
            "relative_performance": "æ•¸æ“šæ”¶é›†ä¸­", 
            "financial_highlights": "æ•¸æ“šæ”¶é›†ä¸­"
        }
        
        # å¾price_dataä¸­æå–
        price_data = key_info.get('price_data', [])
        if price_data:
            for item in price_data:
                content = item.get('content', '').lower()
                
                # æå–åƒ¹æ ¼è®ŠåŒ–è³‡è¨Š
                price_match = re.search(r'(\d+\.?\d*%|\+\d+\.?\d*%|-\d+\.?\d*%)', content)
                if price_match:
                    metrics["price_change"] = f"åƒ¹æ ¼è®Šå‹•: {price_match.group(1)}"
                
                # æå–æˆäº¤é‡è³‡è¨Š
                volume_keywords = ["volume", "trading", "æˆäº¤é‡"]
                if any(keyword in content for keyword in volume_keywords):
                    metrics["volume_analysis"] = "ç™¼ç¾ç•°å¸¸æˆäº¤é‡æ´»å‹•"
        
        # å¾Geminiåˆ†æä¸­æå–
        gemini_analysis = key_info.get('gemini_professional_analysis', '')
        if gemini_analysis:
            # æå–é—œéµæ•¸æ“š
            data_match = re.search(r'é—œéµæ•¸æ“š[ï¼š:]\s*([^ã€‚\n]{20,150})', gemini_analysis)
            if data_match:
                metrics["financial_highlights"] = data_match.group(1)
        
        return metrics
    
    def _extract_enhanced_statements(self, key_info: Dict[str, Any]) -> List[Dict[str, str]]:
        """å¢å¼·ç‰ˆå®˜æ–¹è²æ˜æå–"""
        
        statements = []
        
        # å¾AIæ´è¦‹ä¸­æå–
        ai_insights = key_info.get('rhea_ai_insights', [])
        for insight in ai_insights:
            summary = insight.get('summary', '')
            if summary and len(summary) > 50:
                statements.append({
                    "source": "StockTitan AIåˆ†æ",
                    "quote": summary[:200] + "..." if len(summary) > 200 else summary,
                    "context": f"AIæƒ…ç·’: {insight.get('sentiment', 'N/A')}, å½±éŸ¿: {insight.get('impact', 'N/A')}"
                })
        
        # å¾æ–°èäº‹ä»¶ä¸­æå–
        news_events = key_info.get('news_events', [])
        for event in news_events:
            content = event.get('content', '')
            if content and 'announce' in content.lower() or 'å®£å¸ƒ' in content:
                statements.append({
                    "source": "å…¬å¸å…¬å‘Š",
                    "quote": content[:150] + "..." if len(content) > 150 else content,
                    "context": "ä¾†è‡ªå®˜æ–¹æ–°èç™¼å¸ƒ"
                })
        
        return statements[:3]  # é™åˆ¶æœ€å¤š3å€‹è²æ˜
    
    def _analyze_enhanced_narrative(self, key_info: Dict[str, Any], layer_1: Dict[str, Any]) -> str:
        """å¢å¼·ç‰ˆå¸‚å ´æ•˜äº‹åˆ†æ"""
        
        # æ•´åˆAIæ´è¦‹ä¸­çš„æƒ…ç·’åˆ†æ
        ai_insights = key_info.get('rhea_ai_insights', [])
        if ai_insights:
            sentiments = [insight.get('sentiment', '') for insight in ai_insights if insight.get('sentiment')]
            if sentiments:
                primary_sentiment = sentiments[0]
                narrative = f"å¸‚å ´æƒ…ç·’: {primary_sentiment}ã€‚"
            else:
                narrative = "å¸‚å ´æƒ…ç·’: ä¸­æ€§ã€‚"
        else:
            narrative = "å¸‚å ´æƒ…ç·’: è§€å¯Ÿä¸­ã€‚"
        
        # å¾Geminiåˆ†æä¸­æå–å¸‚å ´åæ‡‰
        gemini_analysis = key_info.get('gemini_professional_analysis', '')
        if gemini_analysis:
            reaction_match = re.search(r'å¸‚å ´åæ‡‰[ï¼š:]\s*([^ã€‚\n]{30,200})', gemini_analysis)
            if reaction_match:
                narrative += f" {reaction_match.group(1)}"
        
        # åˆ†æäº¤æ˜“è¡Œç‚º
        price_data = key_info.get('price_data', [])
        if price_data:
            narrative += " äº¤æ˜“æ´»å‹•é¡¯ç¤ºæŠ•è³‡è€…å°ç›¸é—œäº‹ä»¶çš„ç©æ¥µå›æ‡‰ã€‚"
        
        return narrative
    
    def _analyze_enhanced_comparison(self, key_info: Dict[str, Any]) -> str:
        """å¢å¼·ç‰ˆåŒæ¥­æ¯”è¼ƒ"""
        
        # å¾Geminiåˆ†æä¸­æå–æ¯”è¼ƒè³‡è¨Š
        gemini_analysis = key_info.get('gemini_professional_analysis', '')
        if gemini_analysis:
            comparison_match = re.search(r'(åŒæ¥­|ç«¶çˆ­|æ¯”è¼ƒ)[^ã€‚]*([^ã€‚]{50,200})', gemini_analysis)
            if comparison_match:
                return f"åŒæ¥­æ¯”è¼ƒ: {comparison_match.group(2)}"
        
        # åŸºæ–¼è¡Œæ¥­åˆ†æ
        market_context = key_info.get('market_context', [])
        if market_context:
            return "ç›¸å°æ–¼åŒæ¥­ï¼Œè©²è‚¡ç¥¨è¡¨ç¾ç¬¦åˆè¡Œæ¥­è¶¨å‹¢ã€‚éœ€è¦æ›´å¤šæ•¸æ“šé€²è¡Œè©³ç´°æ¯”è¼ƒã€‚"
        
        return "åŒæ¥­æ¯”è¼ƒè³‡è¨Šæœ‰é™ï¼Œå»ºè­°é—œæ³¨è¡Œæ¥­æ•´é«”è¡¨ç¾ã€‚"
    
    def _analyze_enhanced_consensus(self, key_info: Dict[str, Any]) -> str:
        """å¢å¼·ç‰ˆåˆ†æå¸«å…±è­˜"""
        
        analyst_opinions = key_info.get('analyst_opinions', [])
        
        if analyst_opinions:
            # çµ±è¨ˆåˆ†æå¸«è§€é»
            total_opinions = len(analyst_opinions)
            consensus_items = []
            
            for opinion in analyst_opinions:
                content = opinion.get('content', '').lower()
                
                if 'upgrade' in content or 'buy' in content or 'è²·å…¥' in content:
                    consensus_items.append("æ­£é¢")
                elif 'downgrade' in content or 'sell' in content or 'è³£å‡º' in content:
                    consensus_items.append("è² é¢")
                else:
                    consensus_items.append("ä¸­æ€§")
            
            if consensus_items:
                positive_count = consensus_items.count("æ­£é¢")
                consensus = f"åˆ†æå¸«è§€é»: {total_opinions}ä½åˆ†æå¸«ä¸­ï¼Œ{positive_count}ä½æŒæ­£é¢çœ‹æ³•"
            else:
                consensus = f"æ”¶é›†åˆ°{total_opinions}ä½åˆ†æå¸«è§€é»ï¼Œæ•´é«”è¼ƒç‚ºè¬¹æ…"
        else:
            consensus = "åˆ†æå¸«è¦†è“‹æœ‰é™ï¼Œå»ºè­°é—œæ³¨å®˜æ–¹æŒ‡å¼•"
        
        # å¾Geminiåˆ†æä¸­è£œå……
        gemini_analysis = key_info.get('gemini_professional_analysis', '')
        if gemini_analysis:
            analyst_match = re.search(r'åˆ†æå¸«[^ã€‚]*([^ã€‚]{30,150})', gemini_analysis)
            if analyst_match:
                consensus += f"ã€‚å°ˆæ¥­åˆ†æ: {analyst_match.group(1)}"
        
        return consensus
    
    def _identify_enhanced_bull_case(self, key_info: Dict[str, Any], 
                                   layer_1: Dict[str, Any], 
                                   layer_2: Dict[str, Any]) -> str:
        """å¢å¼·ç‰ˆçœ‹å¤šç†ç”±åˆ†æ"""
        
        bull_points = []
        
        # å¾AIæ´è¦‹ä¸­æå–æ­£é¢å› ç´ 
        ai_insights = key_info.get('rhea_ai_insights', [])
        for insight in ai_insights:
            sentiment = insight.get('sentiment', '').lower()
            impact = insight.get('impact', '').lower()
            
            if 'positive' in sentiment or 'bullish' in sentiment or 'strong' in impact:
                summary = insight.get('summary', '')
                if summary:
                    bull_points.append(summary[:100])
        
        # å¾Geminiåˆ†æä¸­æå–
        gemini_analysis = key_info.get('gemini_professional_analysis', '')
        if gemini_analysis:
            bull_match = re.search(r'(çœ‹å¤š|æ­£é¢|å„ªå‹¢|æ©Ÿæœƒ)[^ã€‚]*([^ã€‚]{50,200})', gemini_analysis)
            if bull_match:
                bull_points.append(bull_match.group(2))
        
        # å¾åŸºæœ¬é¢æ•¸æ“šä¸­æå–
        fundamentals = key_info.get('company_fundamentals', [])
        for item in fundamentals:
            content = item.get('content', '').lower()
            if any(keyword in content for keyword in ['growth', 'profit', 'revenue', 'strong']):
                bull_points.append("åŸºæœ¬é¢è¡¨ç¾å¼·å‹")
                break
        
        if bull_points:
            return "çœ‹å¤šç†ç”±: " + "; ".join(bull_points[:3])
        else:
            return "çœ‹å¤šç†ç”±: éœ€è¦æ›´å¤šæ­£é¢å‚¬åŒ–åŠ‘æ”¯æ’"
    
    def _identify_enhanced_bear_case(self, key_info: Dict[str, Any], 
                                   layer_1: Dict[str, Any], 
                                   layer_2: Dict[str, Any]) -> str:
        """å¢å¼·ç‰ˆçœ‹ç©ºç†ç”±åˆ†æ"""
        
        bear_points = []
        
        # å¾AIæ´è¦‹ä¸­æå–é¢¨éšªå› ç´ 
        ai_insights = key_info.get('rhea_ai_insights', [])
        for insight in ai_insights:
            sentiment = insight.get('sentiment', '').lower()
            impact = insight.get('impact', '').lower()
            
            if 'negative' in sentiment or 'bearish' in sentiment or 'weak' in impact:
                summary = insight.get('summary', '')
                if summary:
                    bear_points.append(summary[:100])
        
        # å¾Geminiåˆ†æä¸­æå–é¢¨éšª
        gemini_analysis = key_info.get('gemini_professional_analysis', '')
        if gemini_analysis:
            risk_match = re.search(r'(é¢¨éšª|æŒ‘æˆ°|æ“”æ†‚|è² é¢)[^ã€‚]*([^ã€‚]{50,200})', gemini_analysis)
            if risk_match:
                bear_points.append(risk_match.group(2))
        
        # é€šç”¨é¢¨éšªå› ç´ 
        bear_points.append("å¸‚å ´æ•´é«”æ³¢å‹•é¢¨éšª")
        
        return "çœ‹ç©ºæ“”æ†‚: " + "; ".join(bear_points[:3])
    
    def _generate_enhanced_outlook(self, key_info: Dict[str, Any], 
                                 layer_1: Dict[str, Any], 
                                 layer_2: Dict[str, Any]) -> str:
        """å¢å¼·ç‰ˆå‰ç»å±•æœ›"""
        
        outlook_points = []
        
        # å¾Geminiåˆ†æä¸­æå–å‰ç»è§€é»
        gemini_analysis = key_info.get('gemini_professional_analysis', '')
        if gemini_analysis:
            outlook_match = re.search(r'(å±•æœ›|æœªä¾†|è¶¨å‹¢|é æœŸ)[^ã€‚]*([^ã€‚]{50,200})', gemini_analysis)
            if outlook_match:
                outlook_points.append(outlook_match.group(2))
        
        # æ™‚é–“æ¡†æ¶åˆ†æ
        outlook_points.extend([
            "çŸ­æœŸ(1-3å€‹æœˆ): é—œæ³¨è²¡å ±å­£è¡¨ç¾å’Œå¸‚å ´æƒ…ç·’",
            "ä¸­æœŸ(3-12å€‹æœˆ): æ³¨æ„è¡Œæ¥­è¶¨å‹¢å’Œç«¶çˆ­æ ¼å±€è®ŠåŒ–",
            "é•·æœŸ: æŒçºŒç›£æ§åŸºæœ¬é¢æ”¹å–„å’Œæˆé•·å‹•èƒ½"
        ])
        
        return "å‰ç»å±•æœ›: " + "; ".join(outlook_points)
    
    def _assess_enhanced_risks(self, key_info: Dict[str, Any], 
                             layer_1: Dict[str, Any], 
                             layer_2: Dict[str, Any]) -> str:
        """å¢å¼·ç‰ˆé¢¨éšªè©•ä¼°"""
        
        risk_categories = {
            "å…¬å¸ç‰¹å®šé¢¨éšª": [],
            "è¡Œæ¥­é¢¨éšª": [],
            "å¸‚å ´é¢¨éšª": ["æ•´é«”å¸‚å ´æ³¢å‹•", "åˆ©ç‡è®ŠåŒ–", "æµå‹•æ€§é¢¨éšª"]
        }
        
        # å¾å…§å®¹ä¸­è­˜åˆ¥é¢¨éšª
        consolidated_content = key_info.get('consolidated_content', '').lower()
        
        risk_keywords = {
            "å…¬å¸ç‰¹å®šé¢¨éšª": ["lawsuit", "debt", "management", "competition"],
            "è¡Œæ¥­é¢¨éšª": ["regulation", "technology", "disruption", "cycle"]
        }
        
        for category, keywords in risk_keywords.items():
            for keyword in keywords:
                if keyword in consolidated_content:
                    risk_categories[category].append(f"{keyword}ç›¸é—œé¢¨éšª")
        
        # æ ¼å¼åŒ–é¢¨éšªè©•ä¼°
        risk_summary = []
        for category, risks in risk_categories.items():
            if risks:
                risk_summary.append(f"{category}: {', '.join(risks[:2])}")
        
        return "; ".join(risk_summary) if risk_summary else "é¢¨éšªè©•ä¼°: ç¶­æŒæ¨™æº–å¸‚å ´é¢¨éšªç›£æ§"
    
    async def _enhance_with_gemini(self, key_info: Dict[str, Any], 
                                 layer_1: Dict[str, Any], 
                                 layer_2: Dict[str, Any], 
                                 layer_3: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨Geminié€²è¡Œé¡å¤–çš„ç¶œåˆåˆ†æå¢å¼·"""
        
        if not self.config.gemini_model:
            return None
        
        try:
            # æ§‹å»ºç¶œåˆåˆ†ææç¤º
            enhancement_prompt = f"""
åŸºæ–¼ä»¥ä¸‹ä¸‰å±¤é‡‘å­—å¡”åˆ†æçµæœï¼Œè«‹æä¾›ç¶œåˆæŠ•è³‡æ´è¦‹ï¼š

ç¬¬ä¸€å±¤ (What - äº‹å¯¦):
{json.dumps(layer_1, ensure_ascii=False, indent=2)}

ç¬¬äºŒå±¤ (Why - æ•˜äº‹):
{json.dumps(layer_2, ensure_ascii=False, indent=2)}

ç¬¬ä¸‰å±¤ (So What - æ´è¦‹):
{json.dumps(layer_3, ensure_ascii=False, indent=2)}

è«‹æä¾›ï¼š
1. æ•´é«”æŠ•è³‡å»ºè­° (è²·å…¥/æŒæœ‰/è³£å‡º)
2. é—œéµé—œæ³¨é»å’Œè§¸ç™¼æ¢ä»¶
3. é¢¨éšªæ”¶ç›Šè©•ä¼°
4. é©åˆçš„æŠ•è³‡è€…é¡å‹

è«‹ä¿æŒå®¢è§€å°ˆæ¥­ï¼Œé¿å…çµ•å°æ€§å»ºè­°ã€‚
"""
            
            response = self.config.gemini_model.generate_content(
                enhancement_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    max_output_tokens=1024
                )
            )
            
            return {
                "comprehensive_analysis": response.text,
                "enhancement_timestamp": datetime.now().isoformat(),
                "success": True
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ Geminiå¢å¼·åˆ†æå¤±æ•—: {str(e)}")
            return {"success": False, "error": str(e)}
    
    # === è¼”åŠ©åŠŸèƒ½å‡½æ•¸ ===
    
    def _count_data_sources(self, key_info: Dict[str, Any]) -> Dict[str, int]:
        """çµ±è¨ˆæ•¸æ“šä¾†æº"""
        return {
            "total_sources": len(key_info.get('rhea_ai_insights', [])),
            "price_data_items": len(key_info.get('price_data', [])),
            "news_items": len(key_info.get('news_events', [])),
            "analyst_opinions": len(key_info.get('analyst_opinions', []))
        }
    
    def _calculate_narrative_confidence(self, key_info: Dict[str, Any]) -> float:
        """è¨ˆç®—æ•˜äº‹ä¿¡å¿ƒåº¦"""
        
        confidence_factors = []
        
        # Geminiåˆ†æå“è³ª
        if key_info.get('gemini_success'):
            confidence_factors.append(0.4)
        
        # AIæ´è¦‹æ•¸é‡
        ai_insights_count = len(key_info.get('rhea_ai_insights', []))
        confidence_factors.append(min(ai_insights_count / 3.0, 0.3))
        
        # æ•¸æ“šè±å¯Œåº¦
        total_data_points = sum([
            len(key_info.get('price_data', [])),
            len(key_info.get('news_events', [])),
            len(key_info.get('analyst_opinions', []))
        ])
        confidence_factors.append(min(total_data_points / 10.0, 0.3))
        
        return min(sum(confidence_factors), 1.0)
    
    def _calculate_thesis_strength(self, key_info: Dict[str, Any], 
                                 layer_1: Dict[str, Any], 
                                 layer_2: Dict[str, Any]) -> float:
        """è¨ˆç®—æŠ•è³‡è«–è¿°å¼·åº¦"""
        
        strength_score = 0.5  # åŸºç¤åˆ†æ•¸
        
        # å‚¬åŒ–åŠ‘æ¸…æ™°åº¦
        catalyst = layer_1.get('core_catalyst', '')
        if 'æª¢æ¸¬åˆ°' in catalyst and 'é‡è¦äº‹ä»¶' in catalyst:
            strength_score += 0.2
        
        # æ•¸æ“šæ”¯æ’åº¦
        content_scores = key_info.get('content_quality_scores', {})
        strength_score += content_scores.get('overall_quality', 0) * 0.3
        
        return min(strength_score, 1.0)
    
    def _assess_analysis_quality(self, layer_1: Dict[str, Any], 
                               layer_2: Dict[str, Any], 
                               layer_3: Dict[str, Any], 
                               key_info: Dict[str, Any]) -> Dict[str, Any]:
        """è©•ä¼°åˆ†æå“è³ª"""
        
        # è¨ˆç®—å®Œæ•´æ€§
        completeness_score = 0.0
        
        # æª¢æŸ¥å„å±¤å…§å®¹è±å¯Œåº¦
        layer_scores = []
        for layer in [layer_1, layer_2, layer_3]:
            layer_content = str(layer)
            if len(layer_content) > 100:  # åŸºæœ¬å…§å®¹é•·åº¦
                layer_scores.append(1.0)
            elif len(layer_content) > 50:
                layer_scores.append(0.5)
            else:
                layer_scores.append(0.0)
        
        completeness_score = sum(layer_scores) / len(layer_scores)
        
        # ç¢ºå®šä¿¡å¿ƒåº¦
        if completeness_score >= 0.8:
            confidence = "high"
        elif completeness_score >= 0.5:
            confidence = "medium"
        else:
            confidence = "low"
        
        return {
            "confidence": confidence,
            "completeness": completeness_score,
            "assumptions": [
                "åŸºæ–¼StockTitanå’ŒGeminiçš„å°ˆæ¥­åˆ†æ",
                "å¸‚å ´æ¢ä»¶å¯èƒ½å¿«é€Ÿè®ŠåŒ–",
                "å»ºè­°çµåˆå…¶ä»–è³‡è¨Šä¾†æºé€²è¡Œæ±ºç­–"
            ]
        }
    
    def _load_analysis_templates(self) -> Dict[str, Any]:
        """è¼‰å…¥åˆ†ææ¨¡æ¿"""
        return {
            "catalyst_patterns": {
                "earnings": ["earnings", "quarterly", "revenue", "profit"],
                "merger": ["merger", "acquisition", "deal"],
                "product": ["product", "launch", "release"],
                "regulatory": ["FDA", "approval", "regulation"]
            },
            "sentiment_indicators": {
                "positive": ["strong", "growth", "beat", "exceed"],
                "negative": ["weak", "decline", "miss", "concern"]
            }
        }

# === èˆ‡ç¾æœ‰ç³»çµ±æ•´åˆçš„ä¸»è¦å‡½æ•¸ ===

# å…¨å±€å¯¦ä¾‹
_analyzer_instance = None

async def process(information_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ç›´æ¥æ›¿æ›åŸæœ¬script_3.pyä¸­çš„processå‡½æ•¸
    å®Œå…¨ç›¸å®¹çš„æ¥å£ï¼Œèƒ½å¤ è™•ç†ä¾†è‡ªscript_2_improved.pyçš„æ–°è³‡æ–™æ ¼å¼
    
    Args:
        information_data: ä¾†è‡ªscript_2_improved.pyçš„è³‡è¨Šæ•¸æ“š
        
    Returns:
        ä¸‰å±¤é‡‘å­—å¡”åˆ†æçµæœï¼Œå¢å¼·ç‰ˆæ ¼å¼
    """
    global _analyzer_instance
    
    if _analyzer_instance is None:
        _analyzer_instance = ImprovedContentAnalyzer()
    
    return await _analyzer_instance.process(information_data)

# æ¸¬è©¦å‡½æ•¸
async def test_improved_analyzer():
    """æ¸¬è©¦æ”¹è‰¯ç‰ˆå…§å®¹åˆ†æå™¨"""
    
    print("ğŸ§ª [YourPods] æ¸¬è©¦æ”¹è‰¯ç‰ˆå…§å®¹åˆ†æå™¨...")
    
    # æ¨¡æ“¬ä¾†è‡ªscript_2_improved.pyçš„æ•¸æ“š
    test_data = {
        "ticker": "AAPL",
        "status": "success",
        "structured_data": {
            "price_data": [{"content": "AAPL price increased 2.5% with high volume"}],
            "news_events": [{"content": "Apple announces new iPhone launch"}],
            "analyst_opinions": [{"content": "Analysts upgrade AAPL to buy rating"}],
            "market_context": [],
            "company_fundamentals": [{"content": "Strong quarterly earnings reported"}]
        },
        "gemini_professional_analysis": {
            "professional_analysis": "Apple shows strong fundamentals with upcoming product launches driving growth. Market sentiment remains positive.",
            "success": True
        },
        "raw_information": [
            {
                "source": "StockTitan_Professional",
                "success": True,
                "rhea_ai_analysis": {
                    "summary": "Apple reports strong quarterly results beating expectations",
                    "sentiment": "Positive",
                    "impact": "Strong positive impact expected"
                }
            }
        ]
    }
    
    try:
        result = await process(test_data)
        
        print(f"âœ… åˆ†æå®Œæˆ!")
        print(f"ç‹€æ…‹: {result.get('status')}")
        print(f"ä¿¡å¿ƒåº¦: {result.get('analysis_metadata', {}).get('confidence_level')}")
        print(f"æ ¸å¿ƒå‚¬åŒ–åŠ‘: {result.get('layer_1_what', {}).get('core_catalyst', 'N/A')[:100]}...")
        
        return result
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_improved_analyzer())
