# éšæ®µ2: æ”¹è‰¯ç‰ˆè³‡è¨Šè’é›† - StockTitan + Gemini 2.5 Pro
# æ›¿ä»£åŸæœ¬çš„ Perplexity API å¯¦ç¾

import asyncio
import json
import logging
import re
import time
import os
from typing import Dict, List, Any, Optional
from datetime import datetime
from dataclasses import dataclass

# Required packages:
# pip install aiohttp firecrawl-py google-generativeai python-dotenv

import aiohttp
import google.generativeai as genai
from firecrawl import FirecrawlApp
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO')),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('YourPods_InformationGathering')

@dataclass
class YourPodsConfig:
    """YourPods å°ˆæ¡ˆé…ç½® - å®‰å…¨çš„APIç®¡ç†"""
    
    def __init__(self):
        # å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥API Keys
        self.firecrawl_api_key = os.getenv('FIRECRAWL_API_KEY')
        self.gemini_api_key = os.getenv('GEMINI_API_KEY')
        self.brave_api_key = os.getenv('BRAVE_API_KEY')  # å¯é¸
        
        # é©—è­‰å¿…è¦çš„API Keys
        if not self.firecrawl_api_key:
            raise ValueError("âŒ FIRECRAWL_API_KEY ç’°å¢ƒè®Šæ•¸æœªè¨­ç½® - è«‹æª¢æŸ¥ .env æª”æ¡ˆ")
        
        if not self.gemini_api_key:
            raise ValueError("âŒ GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸æœªè¨­ç½® - è«‹æª¢æŸ¥ .env æª”æ¡ˆ")
        
        # YourPods ç³»çµ±é…ç½®
        self.stocktitan_base = "https://www.stocktitan.net"
        self.cache_duration_hours = int(os.getenv('CACHE_DURATION_HOURS', '2'))
        self.max_content_length = int(os.getenv('MAX_CONTENT_LENGTH', '30000'))
        self.request_timeout = int(os.getenv('REQUEST_TIMEOUT', '15000'))
        
        # æˆæœ¬æ§åˆ¶
        self.daily_api_limit = int(os.getenv('DAILY_API_LIMIT', '100'))
        self.hourly_api_limit = int(os.getenv('HOURLY_API_LIMIT', '20'))
        
        logger.info("âœ… YourPodsé…ç½®è¼‰å…¥æˆåŠŸ - StockTitan + Gemini 2.5 Pro")

class ImprovedInformationGatherer:
    """æ”¹è‰¯ç‰ˆè³‡è¨Šè’é›†éšæ®µ - å®Œå…¨æ›¿ä»£ Perplexity API"""
    
    def __init__(self):
        # è¼‰å…¥é…ç½®
        self.config = YourPodsConfig()
        
        # åˆå§‹åŒ–æœå‹™
        self.firecrawl = FirecrawlApp(api_key=self.config.firecrawl_api_key)
        
        # é…ç½®Gemini
        genai.configure(api_key=self.config.gemini_api_key)
        self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
        
        # å¿«å–å’Œä½¿ç”¨é‡è¿½è¹¤
        self.cache = {}
        self.api_usage_tracker = {
            'daily_calls': 0,
            'hourly_calls': 0,
            'last_reset_day': datetime.now().day,
            'last_reset_hour': datetime.now().hour
        }
        
        logger.info("ğŸš€ YourPods æ”¹è‰¯ç‰ˆè³‡è¨Šæ”¶é›†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def process(self, stock_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸»è¦è™•ç†å‡½æ•¸ - èˆ‡åŸæœ¬ script_2.py å®Œå…¨ç›¸å®¹
        
        Args:
            stock_data: ä¾†è‡ª script_1.py çš„è‚¡ç¥¨è³‡æ–™
            
        Returns:
            èˆ‡åŸæœ¬ script_2.py ç›¸åŒæ ¼å¼çš„çµæœï¼Œä½†ä½¿ç”¨ StockTitan + Gemini
        """
        ticker = stock_data['standardized_ticker']
        company_name = stock_data.get('company_name', '')
        industry = stock_data.get('industry', '')
        
        logger.info(f"ğŸ¯ [YourPods] é–‹å§‹æ”¶é›† {ticker} ({company_name}) çš„å°ˆæ¥­è²¡ç¶“è³‡è¨Š")
        
        try:
            # 1. æª¢æŸ¥APIä½¿ç”¨é™åˆ¶
            if not self._check_api_limits():
                return self._create_error_response(ticker, "APIä½¿ç”¨é‡å·²é”åˆ°æ¯æ—¥/æ¯å°æ™‚é™åˆ¶")
            
            # 2. æª¢æŸ¥å¿«å–
            cached_result = self._check_cache(ticker)
            if cached_result:
                logger.info(f"ğŸ“‹ ä½¿ç”¨å¿«å–è³‡æ–™: {ticker}")
                return cached_result
            
            # 3. æŠ“å– StockTitan å°ˆæ¥­è³‡æ–™
            stocktitan_data = await self._fetch_professional_data(ticker, company_name, industry)
            
            # 4. å“è³ªæª¢æŸ¥ï¼Œå¿…è¦æ™‚è£œå……å‚™ç”¨ä¾†æº
            if not self._is_data_sufficient(stocktitan_data):
                logger.info(f"ğŸ“¡ è³‡æ–™ä¸è¶³ï¼Œå•Ÿç”¨å‚™ç”¨ä¾†æº...")
                backup_data = await self._fetch_backup_sources(ticker)
                stocktitan_data.extend(backup_data)
            
            # 5. ä½¿ç”¨ Gemini 2.5 Pro é€²è¡Œå°ˆæ¥­åˆ†æ
            gemini_analysis = await self._analyze_with_gemini_pro(ticker, stocktitan_data, industry)
            
            # 6. çµæ§‹åŒ–è¼¸å‡º (èˆ‡åŸæœ¬ script_2.py æ ¼å¼å®Œå…¨ç›¸å®¹)
            result = self._format_compatible_result(ticker, stocktitan_data, gemini_analysis)
            
            # 7. æ›´æ–°å¿«å–å’Œä½¿ç”¨é‡
            self._update_cache(ticker, result)
            self._update_api_usage()
            
            logger.info(f"âœ… {ticker} å°ˆæ¥­è³‡è¨Šæ”¶é›†å®Œæˆ - ä¾†æºæ•¸: {len(stocktitan_data)}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ {ticker} è³‡è¨Šæ”¶é›†å¤±æ•—: {str(e)}")
            return self._create_error_response(ticker, str(e))
    
    async def _fetch_professional_data(self, ticker: str, company_name: str, industry: str) -> List[Dict[str, Any]]:
        """æŠ“å– StockTitan çš„å°ˆæ¥­è²¡ç¶“è³‡æ–™"""
        
        # æ§‹å»ºæ™ºèƒ½URLç­–ç•¥
        primary_urls = [
            f"{self.config.stocktitan_base}/news/{ticker}/",  # å€‹è‚¡å°ˆé 
            f"{self.config.stocktitan_base}/news/today",      # ä»Šæ—¥å¸‚å ´æ–°è
        ]
        
        # æ ¹æ“šå¸‚å ´ç‹€æ…‹å’Œæ™‚é–“åŠ å…¥é¡å¤–ä¾†æº
        if self._is_market_hours():
            primary_urls.append(f"{self.config.stocktitan_base}/news/live.html")
        
        if self._is_earnings_season():
            primary_urls.append(f"{self.config.stocktitan_base}/news/earnings.html")
        
        # ä¸¦è¡ŒæŠ“å–æ‰€æœ‰ä¾†æº
        tasks = [self._scrape_stocktitan_url(url, ticker) for url in primary_urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # éæ¿¾å’Œæ•´ç†æˆåŠŸçµæœ
        successful_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.warning(f"âš ï¸ URL {primary_urls[i]} æŠ“å–å¤±æ•—: {str(result)}")
            elif result and result.get('success'):
                successful_results.append(result)
        
        logger.info(f"ğŸ“Š æˆåŠŸæŠ“å– {len(successful_results)}/{len(primary_urls)} å€‹StockTitanä¾†æº")
        return successful_results
    
    async def _scrape_stocktitan_url(self, url: str, ticker: str) -> Dict[str, Any]:
        """æŠ“å–å–®å€‹ StockTitan URL"""
        
        try:
            logger.info(f"ğŸŒ æŠ“å–å°ˆæ¥­è²¡ç¶“ä¾†æº: {url}")
            
            # ä½¿ç”¨ Firecrawl é€²è¡Œå°ˆæ¥­ç¶²é æŠ“å–
            result = self.firecrawl.scrape_url(
                url=url,
                params={
                    'formats': ['markdown'],
                    'onlyMainContent': True,
                    'removeBase64Images': True,
                    'timeout': self.config.request_timeout,
                    'waitFor': 2000  # ç­‰å¾…å‹•æ…‹å…§å®¹è¼‰å…¥
                }
            )
            
            if result.get('success'):
                content = result.get('markdown', '')
                
                # æ™ºèƒ½æå–èˆ‡ç›®æ¨™è‚¡ç¥¨ç›¸é—œçš„å…§å®¹
                relevant_content = self._extract_intelligent_content(content, ticker)
                
                if relevant_content:
                    # æå– StockTitan çš„ Rhea-AI å°ˆæ¥­åˆ†ææ•¸æ“š
                    rhea_ai_data = self._extract_rhea_ai_analysis(content)
                    
                    return {
                        'url': url,
                        'source': 'StockTitan_Professional',
                        'raw_content': content,
                        'relevant_content': relevant_content,
                        'rhea_ai_analysis': rhea_ai_data,
                        'timestamp': datetime.now().isoformat(),
                        'success': True,
                        'quality_score': self._calculate_content_quality(relevant_content, rhea_ai_data)
                    }
                else:
                    logger.info(f"ğŸ“­ {url} æ²’æœ‰æ‰¾åˆ° {ticker} çš„ç›¸é—œå°ˆæ¥­å…§å®¹")
                    return {'url': url, 'success': False, 'reason': 'No relevant professional content'}
            else:
                return {'url': url, 'success': False, 'error': 'FirecrawlæŠ“å–å¤±æ•—'}
                
        except Exception as e:
            logger.error(f"âŒ æŠ“å– {url} å¤±æ•—: {str(e)}")
            return {'url': url, 'success': False, 'error': str(e)}
    
    def _extract_intelligent_content(self, content: str, ticker: str) -> str:
        """æ™ºèƒ½æå–èˆ‡è‚¡ç¥¨ç›¸é—œçš„å…§å®¹"""
        
        if not content:
            return ""
        
        # åˆ†å‰²å…§å®¹ç‚ºæ®µè½å’Œå¥å­
        paragraphs = content.split('\n\n')
        relevant_paragraphs = []
        
        # å¤šå±¤æ¬¡æœç´¢æ¨¡å¼
        primary_patterns = [
            ticker.upper(),
            ticker.lower(),
            f"({ticker})",
            f"NYSE: {ticker}",
            f"NASDAQ: {ticker}",
            f"${ticker}"
        ]
        
        # è²¡ç¶“ç›¸é—œé—œéµå­—
        financial_keywords = [
            'earnings', 'revenue', 'profit', 'guidance', 'analyst',
            'rating', 'target', 'price', 'volume', 'trading',
            'quarterly', 'annual', 'financial', 'results'
        ]
        
        for paragraph in paragraphs:
            # æª¢æŸ¥æ˜¯å¦åŒ…å«ç›®æ¨™è‚¡ç¥¨
            contains_ticker = any(pattern in paragraph for pattern in primary_patterns)
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«è²¡ç¶“é—œéµå­—
            contains_financial = any(keyword in paragraph.lower() for keyword in financial_keywords)
            
            if contains_ticker or (contains_financial and len(paragraph) > 100):
                relevant_paragraphs.append(paragraph.strip())
        
        result = '\n\n'.join(relevant_paragraphs)
        logger.info(f"ğŸ“ ç‚º {ticker} æ™ºèƒ½æå–äº† {len(relevant_paragraphs)} å€‹å°ˆæ¥­æ®µè½")
        
        return result
    
    def _extract_rhea_ai_analysis(self, content: str) -> Dict[str, str]:
        """æå– StockTitan çš„ Rhea-AI å°ˆæ¥­åˆ†ææ•¸æ“š"""
        
        rhea_analysis = {
            "summary": "",
            "sentiment": "",
            "impact": "",
            "end_of_day": "",
            "tags": []
        }
        
        # æ”¹è‰¯çš„æ­£å‰‡è¡¨é”å¼æ¨¡å¼
        patterns = {
            "summary": r"Rhea-AI Summary[:\s]*(.*?)(?=\n\n|Rhea-AI|Tags|$)",
            "sentiment": r"Rhea-AI Sentiment[:\s]*(.*?)(?=\n\n|Rhea-AI|Tags|$)",
            "impact": r"Rhea-AI Impact[:\s]*(.*?)(?=\n\n|Rhea-AI|Tags|$)",
            "end_of_day": r"End-of-Day[:\s]*(.*?)(?=\n\n|Rhea-AI|Tags|$)"
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
            if match:
                rhea_analysis[key] = match.group(1).strip()
        
        # æå–æ¨™ç±¤
        tags_match = re.search(r"Tags[:\s]*(.*?)(?=\n\n|$)", content, re.DOTALL | re.IGNORECASE)
        if tags_match:
            tags_text = tags_match.group(1).strip()
            rhea_analysis["tags"] = [tag.strip() for tag in tags_text.split() if tag.strip()]
        
        # è¨˜éŒ„æ‰¾åˆ°çš„å°ˆæ¥­AIæ•¸æ“š
        found_data = [k for k, v in rhea_analysis.items() if v and k != 'tags']
        if found_data:
            logger.info(f"ğŸ¤– æ‰¾åˆ°Rhea-AIå°ˆæ¥­åˆ†æ: {', '.join(found_data)}")
        
        return rhea_analysis
    
    async def _fetch_backup_sources(self, ticker: str) -> List[Dict[str, Any]]:
        """å‚™ç”¨è³‡æ–™ä¾†æº"""
        
        backup_sources = [
            f"https://finance.yahoo.com/quote/{ticker}/news/",
            f"https://www.marketwatch.com/investing/stock/{ticker.lower()}"
        ]
        
        logger.info(f"ğŸ”„ å•Ÿç”¨å‚™ç”¨è²¡ç¶“ä¾†æº...")
        
        backup_results = []
        for url in backup_sources[:1]:  # æ§åˆ¶æˆæœ¬ï¼Œåªä½¿ç”¨1å€‹å‚™ç”¨ä¾†æº
            try:
                result = self.firecrawl.scrape_url(
                    url=url,
                    params={
                        'formats': ['markdown'],
                        'onlyMainContent': True,
                        'timeout': 10000
                    }
                )
                
                if result.get('success'):
                    backup_results.append({
                        'url': url,
                        'source': 'Backup_Financial',
                        'raw_content': result.get('markdown', ''),
                        'relevant_content': self._extract_intelligent_content(result.get('markdown', ''), ticker),
                        'timestamp': datetime.now().isoformat(),
                        'success': True
                    })
                    
            except Exception as e:
                logger.warning(f"âš ï¸ å‚™ç”¨ä¾†æº {url} å¤±æ•—: {str(e)}")
        
        return backup_results
    
    async def _analyze_with_gemini_pro(self, ticker: str, data_sources: List[Dict[str, Any]], industry: str) -> Dict[str, Any]:
        """ä½¿ç”¨ Gemini 2.5 Pro é€²è¡Œå°ˆæ¥­è²¡ç¶“åˆ†æ"""
        
        # æ•´åˆæ‰€æœ‰å°ˆæ¥­å…§å®¹
        combined_analysis = self._combine_professional_content(data_sources)
        
        if not combined_analysis.strip():
            return {"error": "æ²’æœ‰è¶³å¤ çš„å°ˆæ¥­å…§å®¹é€²è¡Œåˆ†æ", "success": False}
        
        # æ§‹å»ºå°ˆæ¥­è²¡ç¶“åˆ†ææç¤º
        professional_prompt = f"""
ä½ æ˜¯é ‚ç´šçš„è¯çˆ¾è¡—è²¡ç¶“åˆ†æå¸«ï¼Œå°ˆé–€åˆ†æç¾è‚¡å¸‚å ´ã€‚è«‹å°è‚¡ç¥¨ {ticker} é€²è¡Œå°ˆæ¥­åˆ†æã€‚

è¡Œæ¥­èƒŒæ™¯: {industry}

å¯ç”¨çš„å°ˆæ¥­è³‡æ–™:
{combined_analysis}

è«‹æä¾›çµæ§‹åŒ–çš„å°ˆæ¥­åˆ†æï¼ŒåŒ…æ‹¬ï¼š

## 1. æ ¸å¿ƒå‚¬åŒ–åŠ‘ (Key Catalyst)
- è­˜åˆ¥æœ€é‡è¦çš„åƒ¹æ ¼é©…å‹•äº‹ä»¶
- è©•ä¼°äº‹ä»¶çš„é‡è¦æ€§å’Œæ™‚æ•ˆæ€§

## 2. å¸‚å ´æƒ…ç·’åˆ†æ (Market Sentiment)
- æŠ•è³‡è€…åæ‡‰å’Œæƒ…ç·’æŒ‡æ¨™
- äº¤æ˜“é‡å’Œåƒ¹æ ¼è¡Œç‚ºåˆ†æ

## 3. åŸºæœ¬é¢è©•ä¼° (Fundamental Analysis)
- é—œéµè²¡å‹™æŒ‡æ¨™å’Œè¡¨ç¾
- èˆ‡åŒæ¥­ç«¶çˆ­å°æ‰‹æ¯”è¼ƒ

## 4. åˆ†æå¸«è§€é»åŒ¯ç¸½ (Analyst Consensus)
- å°ˆæ¥­æ©Ÿæ§‹çš„è©•ç´šå’Œç›®æ¨™åƒ¹
- è¿‘æœŸè©•ç´šè®ŠåŒ–å’Œç†ç”±

## 5. é¢¨éšªè©•ä¼° (Risk Assessment)
- ä¸»è¦é¢¨éšªå› ç´ è­˜åˆ¥
- ä¸Šæª”å’Œä¸‹æª”ç›®æ¨™

## 6. æŠ•è³‡å»ºè­° (Investment Thesis)
- çŸ­æœŸ (1-3å€‹æœˆ) å±•æœ›
- ä¸­æœŸ (3-12å€‹æœˆ) è¶¨å‹¢

è¦æ±‚:
- åŸºæ–¼å…·é«”äº‹å¯¦å’Œæ•¸æ“š
- ä¿æŒå®¢è§€ä¸­æ€§çš„å°ˆæ¥­è§’åº¦
- ä½¿ç”¨å°ˆæ¥­è²¡ç¶“è¡“èª
- å¦‚æœè³‡è¨Šä¸è¶³ï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºé™åˆ¶
"""

        try:
            logger.info(f"ğŸ¤– ä½¿ç”¨ Gemini 2.5 Pro åˆ†æ {ticker}...")
            
            response = self.gemini_model.generate_content(
                professional_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.2,  # è¼ƒä½æº«åº¦ç¢ºä¿å°ˆæ¥­æº–ç¢ºæ€§
                    top_p=0.8,
                    top_k=40,
                    max_output_tokens=2048
                )
            )
            
            return {
                "professional_analysis": response.text,
                "model_used": "gemini-2.0-flash-exp",
                "analysis_type": "professional_financial",
                "timestamp": datetime.now().isoformat(),
                "success": True,
                "content_processed": len(combined_analysis),
                "industry_context": industry
            }
            
        except Exception as e:
            logger.error(f"âŒ Gemini 2.5 Pro åˆ†æå¤±æ•—: {str(e)}")
            return {
                "error": f"Geminiå°ˆæ¥­åˆ†æå¤±æ•—: {str(e)}",
                "success": False
            }
    
    def _combine_professional_content(self, data_sources: List[Dict[str, Any]]) -> str:
        """æ•´åˆå°ˆæ¥­å…§å®¹ï¼Œå„ªå…ˆè™•ç†AIåˆ†ææ•¸æ“š"""
        
        content_sections = []
        
        for source in data_sources:
            if not source.get('success'):
                continue
            
            # å„ªå…ˆè™•ç† Rhea-AI å°ˆæ¥­åˆ†æ
            rhea_analysis = source.get('rhea_ai_analysis', {})
            if rhea_analysis and any(rhea_analysis.values()):
                ai_sections = []
                
                if rhea_analysis.get('summary'):
                    ai_sections.append(f"AIå°ˆæ¥­æ‘˜è¦: {rhea_analysis['summary']}")
                
                if rhea_analysis.get('sentiment'):
                    ai_sections.append(f"AIæƒ…ç·’åˆ†æ: {rhea_analysis['sentiment']}")
                
                if rhea_analysis.get('impact'):
                    ai_sections.append(f"AIå½±éŸ¿è©•ä¼°: {rhea_analysis['impact']}")
                
                if rhea_analysis.get('end_of_day'):
                    ai_sections.append(f"æ”¶ç›¤æ•¸æ“š: {rhea_analysis['end_of_day']}")
                
                if ai_sections:
                    content_sections.append("=== StockTitan AIå°ˆæ¥­åˆ†æ ===\n" + "\n".join(ai_sections))
            
            # æ·»åŠ ç›¸é—œå°ˆæ¥­å…§å®¹
            relevant_content = source.get('relevant_content', '')
            if relevant_content:
                source_name = source.get('source', 'Unknown')
                quality_score = source.get('quality_score', 0)
                content_sections.append(f"=== {source_name} (å“è³ª: {quality_score:.1f}) ===\n{relevant_content}")
        
        combined = "\n\n".join(content_sections)
        
        # æ™ºèƒ½æˆªæ–·ï¼Œä¿ç•™æœ€é‡è¦çš„å…§å®¹
        if len(combined) > self.config.max_content_length:
            # å„ªå…ˆä¿ç•™AIåˆ†æå’Œé«˜å“è³ªå…§å®¹
            truncated = combined[:self.config.max_content_length]
            combined = truncated + "\n\n[å…§å®¹å› é•·åº¦é™åˆ¶è¢«æ™ºèƒ½æˆªæ–·ï¼Œå·²ä¿ç•™æœ€é‡è¦éƒ¨åˆ†]"
        
        return combined
    
    def _format_compatible_result(self, ticker: str, raw_data: List[Dict[str, Any]], 
                                 gemini_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¼å¼åŒ–çµæœï¼Œèˆ‡åŸæœ¬ script_2.py å®Œå…¨ç›¸å®¹"""
        
        # å‰µå»ºèˆ‡åŸæœ¬ script_2.py ç›¸åŒçš„ structured_data æ ¼å¼
        structured_data = {
            "price_data": [],
            "news_events": [],
            "analyst_opinions": [],
            "market_context": [],
            "company_fundamentals": []
        }
        
        # æ™ºèƒ½å…§å®¹åˆ†é¡
        for source in raw_data:
            if not source.get('success'):
                continue
                
            content = source.get('relevant_content', '')
            rhea_data = source.get('rhea_ai_analysis', {})
            
            # åŸºæ–¼é—œéµå­—å’ŒAIæ•¸æ“šé€²è¡Œåˆ†é¡
            content_item = {
                "content": content[:800],  # é™åˆ¶é•·åº¦
                "source": source.get('url', ''),
                "timestamp": source.get('timestamp', ''),
                "quality_score": source.get('quality_score', 0),
                "ai_analysis": rhea_data
            }
            
            # åƒ¹æ ¼å’Œäº¤æ˜“æ•¸æ“š
            if any(keyword in content.lower() for keyword in ['price', 'trading', 'volume', 'market']):
                structured_data["price_data"].append(content_item)
            
            # åˆ†æå¸«è§€é»
            if any(keyword in content.lower() for keyword in ['analyst', 'rating', 'target', 'upgrade', 'downgrade']):
                structured_data["analyst_opinions"].append(content_item)
            
            # æ–°èäº‹ä»¶
            if any(keyword in content.lower() for keyword in ['announce', 'report', 'release', 'launch']):
                structured_data["news_events"].append(content_item)
            
            # åŸºæœ¬é¢æ•¸æ“š
            if any(keyword in content.lower() for keyword in ['earnings', 'revenue', 'profit', 'guidance']):
                structured_data["company_fundamentals"].append(content_item)
            
            # å¸‚å ´èƒŒæ™¯
            else:
                structured_data["market_context"].append(content_item)
        
        # å‰µå»ºèˆ‡åŸæœ¬æ ¼å¼ç›¸å®¹çš„è¿”å›çµæœ
        return {
            "status": "success",
            "ticker": ticker,
            "raw_information": raw_data,
            "structured_data": structured_data,
            "gemini_professional_analysis": gemini_analysis,  # æ–°å¢çš„å°ˆæ¥­åˆ†æ
            "collection_metadata": {
                "timestamp": datetime.now().isoformat(),
                "data_sources": len(raw_data),
                "analysis_success": gemini_analysis.get('success', False),
                "total_content_length": sum(len(s.get('relevant_content', '')) for s in raw_data),
                "ai_analysis_found": sum(1 for s in raw_data if s.get('rhea_ai_analysis', {}).get('summary')),
                "method": "StockTitan_Gemini_Pro",
                "cost_estimate": self._calculate_processing_cost(raw_data, gemini_analysis)
            }
        }
    
    def _calculate_content_quality(self, content: str, rhea_ai_data: Dict[str, str]) -> float:
        """è¨ˆç®—å…§å®¹å“è³ªåˆ†æ•¸"""
        
        score = 0.0
        
        # åŸºç¤å…§å®¹é•·åº¦åˆ†æ•¸
        if len(content) > 200:
            score += 0.3
        elif len(content) > 500:
            score += 0.5
        
        # AIåˆ†ææ•¸æ“šåŠ åˆ†
        ai_fields_found = sum(1 for v in rhea_ai_data.values() if v and isinstance(v, str))
        score += ai_fields_found * 0.15
        
        # å°ˆæ¥­é—œéµå­—åŠ åˆ†
        professional_keywords = ['earnings', 'revenue', 'analyst', 'target', 'guidance']
        keyword_score = sum(0.1 for keyword in professional_keywords if keyword in content.lower())
        score += min(keyword_score, 0.3)
        
        return min(score, 1.0)
    
    def _calculate_processing_cost(self, raw_data: List[Dict[str, Any]], 
                                 gemini_analysis: Dict[str, Any]) -> float:
        """è¨ˆç®—è™•ç†æˆæœ¬"""
        
        firecrawl_cost = len(raw_data) * 0.5  # $0.5 per page
        gemini_cost = 0.02 if gemini_analysis.get('success') else 0  # ~$0.02 per analysis
        
        return round(firecrawl_cost + gemini_cost, 3)
    
    def _is_data_sufficient(self, data: List[Dict[str, Any]]) -> bool:
        """è©•ä¼°è³‡æ–™å……åˆ†æ€§"""
        
        if not data:
            return False
        
        # è¨ˆç®—ç¸½å…§å®¹é•·åº¦
        total_content = sum(len(source.get('relevant_content', '')) for source in data)
        
        # æª¢æŸ¥AIåˆ†ææ•¸æ“š
        ai_analysis_count = sum(1 for source in data 
                               if source.get('rhea_ai_analysis', {}).get('summary'))
        
        # è¨ˆç®—å¹³å‡å“è³ªåˆ†æ•¸
        quality_scores = [source.get('quality_score', 0) for source in data if source.get('success')]
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        sufficient = (total_content > 800 and avg_quality > 0.5) or ai_analysis_count > 0
        
        logger.info(f"ğŸ“Š è³‡æ–™å……åˆ†æ€§è©•ä¼°: å…§å®¹={total_content}å­—, AIåˆ†æ={ai_analysis_count}å€‹, "
                   f"å¹³å‡å“è³ª={avg_quality:.2f}, çµæœ={'å……åˆ†' if sufficient else 'ä¸è¶³'}")
        
        return sufficient
    
    def _create_error_response(self, ticker: str, error_message: str) -> Dict[str, Any]:
        """å‰µå»ºéŒ¯èª¤éŸ¿æ‡‰"""
        
        return {
            "status": "error",
            "ticker": ticker,
            "error_message": error_message,
            "raw_information": [],
            "structured_data": {
                "price_data": [],
                "news_events": [],
                "analyst_opinions": [],
                "market_context": [],
                "company_fundamentals": []
            },
            "collection_metadata": {
                "timestamp": datetime.now().isoformat(),
                "error": True,
                "method": "StockTitan_Gemini_Pro"
            }
        }
    
    # === è¼”åŠ©åŠŸèƒ½å‡½æ•¸ ===
    
    def _check_api_limits(self) -> bool:
        """æª¢æŸ¥APIä½¿ç”¨é™åˆ¶"""
        
        now = datetime.now()
        
        # é‡ç½®æ¯æ—¥è¨ˆæ•¸
        if now.day != self.api_usage_tracker['last_reset_day']:
            self.api_usage_tracker['daily_calls'] = 0
            self.api_usage_tracker['last_reset_day'] = now.day
        
        # é‡ç½®æ¯å°æ™‚è¨ˆæ•¸
        if now.hour != self.api_usage_tracker['last_reset_hour']:
            self.api_usage_tracker['hourly_calls'] = 0
            self.api_usage_tracker['last_reset_hour'] = now.hour
        
        # æª¢æŸ¥é™åˆ¶
        daily_ok = self.api_usage_tracker['daily_calls'] < self.config.daily_api_limit
        hourly_ok = self.api_usage_tracker['hourly_calls'] < self.config.hourly_api_limit
        
        return daily_ok and hourly_ok
    
    def _update_api_usage(self):
        """æ›´æ–°APIä½¿ç”¨é‡"""
        self.api_usage_tracker['daily_calls'] += 1
        self.api_usage_tracker['hourly_calls'] += 1
    
    def _check_cache(self, ticker: str) -> Optional[Dict[str, Any]]:
        """æª¢æŸ¥å¿«å–"""
        if ticker in self.cache:
            cached_time, cached_data = self.cache[ticker]
            hours_elapsed = (time.time() - cached_time) / 3600
            
            if hours_elapsed < self.config.cache_duration_hours:
                return cached_data
        
        return None
    
    def _update_cache(self, ticker: str, data: Dict[str, Any]):
        """æ›´æ–°å¿«å–"""
        self.cache[ticker] = (time.time(), data)
    
    def _is_market_hours(self) -> bool:
        """æª¢æŸ¥ç¾åœ‹å¸‚å ´äº¤æ˜“æ™‚é–“"""
        now = datetime.now()
        # ç°¡åŒ–åˆ¤æ–·ï¼šé€±ä¸€åˆ°é€±äº”çš„9-16é» (å¯ä»¥æ”¹ç”¨pytzåšç²¾ç¢ºæ™‚å€è½‰æ›)
        return 9 <= now.hour <= 16 and now.weekday() < 5
    
    def _is_earnings_season(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºè²¡å ±å­£"""
        now = datetime.now()
        # è²¡å ±å­£é€šå¸¸æ˜¯æ¯å­£åº¦çš„å‰6é€±
        month = now.month
        return month in [1, 2, 4, 5, 7, 8, 10, 11]

# === èˆ‡ç¾æœ‰ç³»çµ±æ•´åˆçš„ä¸»è¦å‡½æ•¸ ===

# å…¨å±€å¯¦ä¾‹ (å–®ä¾‹æ¨¡å¼)
_gatherer_instance = None

async def process(stock_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ç›´æ¥æ›¿æ›åŸæœ¬ script_2.py ä¸­çš„ process å‡½æ•¸
    å®Œå…¨ç›¸å®¹çš„æ¥å£ï¼Œç„¡éœ€ä¿®æ”¹å…¶ä»–ä»£ç¢¼
    
    Args:
        stock_data: ä¾†è‡ª script_1.py çš„è‚¡ç¥¨è³‡æ–™
        
    Returns:
        èˆ‡åŸæœ¬ script_2.py ç›¸åŒæ ¼å¼çš„çµæœï¼Œä½†ä½¿ç”¨ StockTitan + Gemini 2.5 Pro
    """
    global _gatherer_instance
    
    # å»¶é²åˆå§‹åŒ– (é¿å…å°å…¥æ™‚çš„éŒ¯èª¤)
    if _gatherer_instance is None:
        _gatherer_instance = ImprovedInformationGatherer()
    
    return await _gatherer_instance.process(stock_data)

# æ¸¬è©¦å’Œé©—è­‰å‡½æ•¸
async def test_improved_gatherer():
    """æ¸¬è©¦æ”¹è‰¯ç‰ˆè³‡è¨Šæ”¶é›†å™¨"""
    
    print("ğŸ§ª [YourPods] æ¸¬è©¦æ”¹è‰¯ç‰ˆè³‡è¨Šæ”¶é›†å™¨...")
    
    # æª¢æŸ¥ç’°å¢ƒé…ç½®
    try:
        config = YourPodsConfig()
        print("âœ… ç’°å¢ƒé…ç½®æª¢æŸ¥é€šé")
    except ValueError as e:
        print(f"âŒ é…ç½®éŒ¯èª¤: {e}")
        print("è«‹ç¢ºä¿ .env æª”æ¡ˆåŒ…å«å¿…è¦çš„API Keys")
        return
    
    # æ¨¡æ“¬ä¾†è‡ª script_1.py çš„è¼¸å…¥
    test_data = {
        "standardized_ticker": "AAPL",
        "company_name": "Apple Inc.",
        "industry": "Technology Hardware",
        "market_status": "open",
        "current_price": 175.0,
        "currency": "USD"
    }
    
    try:
        print(f"ğŸ“Š æ¸¬è©¦è‚¡ç¥¨: {test_data['standardized_ticker']}")
        
        # æ¸¬è©¦æ–°çš„processå‡½æ•¸
        result = await process(test_data)
        
        print(f"\nâœ… æ¸¬è©¦å®Œæˆ!")
        print(f"ç‹€æ…‹: {result.get('status')}")
        print(f"è³‡æ–™ä¾†æº: {result.get('collection_metadata', {}).get('data_sources', 0)}å€‹")
        print(f"è™•ç†æ–¹æ³•: {result.get('collection_metadata', {}).get('method', 'Unknown')}")
        
        if result.get('gemini_professional_analysis', {}).get('success'):
            analysis = result['gemini_professional_analysis']['professional_analysis']
            print(f"ğŸ¤– Gemini 2.5 Proåˆ†ææˆåŠŸ")
            print(f"åˆ†æé è¦½: {analysis[:150]}...")
        
        # ä¼°ç®—æˆæœ¬
        cost = result.get('collection_metadata', {}).get('cost_estimate', 0)
        print(f"ğŸ’° é ä¼°æˆæœ¬: ${cost}")
        
        return result
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_improved_gatherer())
